{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal** of this competition is to predict which clients are more likely to default on their loans. The evaluation is based on gini stability metric and will favor solutions that are stable over time. A separate chalenge is to deal with large data sizes and constantly monitor and reduce memory usage to not exceed allocated amount of RAM.  \n",
    "\n",
    "**Structure**\n",
    "\n",
    "There are several tables classified by 'depth':\n",
    "* depth=0 - These are static features directly tied to a specific case_id.\n",
    "* depth=1 - Each case_id has an associated historical record, indexed by num_group1.\n",
    "* depth=2 - Each case_id has an associated historical record, indexed by both num_group1 and num_group2.\n",
    "\n",
    "Various predictors were transformed, therefore we have the following notation for similar groups of transformations\n",
    "\n",
    "* P - Transform DPD (Days past due)\n",
    "* M - Masking categories\n",
    "* A - Transform amount\n",
    "* D - Transform date\n",
    "* T - Unspecified Transform\n",
    "* L - Unspecified Transform\n",
    "\n",
    "Transformations within a group are denoted by a capital letter at the end of the predictor name (e.g., maxdbddpdtollast6m_4187119P)\n",
    "\n",
    "**Strategy**\n",
    "\n",
    "Following approach for data processing will be used:\n",
    "* depth=2 files will be aggregated, grouped by case_id and num_group1. For numerical columns we will calculate the average between num_group2 values and for categoricals we will get the most frequent value. Like this they will become as depth=1 files and the same processing function as for depth=1 files will be applied to them.\n",
    "* for depth=1 files we will use an aggregation grouped by case_id. For numerical columns we will calculate the mean, std, min and max between num_group1 values. For categoricals we will get the most frequent value. After this the files will become as depth=0.\n",
    "* at depth=0 level we will merge all the files together on case_id as all the case_id are unique at this level. \n",
    "\n",
    "After data analysis, time delta features will be created based on timeseries columns and all the preparations for modeling will be made using a processing pipeline. Competition's stability metric will be integrated during Optuna tuning process and also in the model training as evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-19T11:31:39.638962Z",
     "iopub.status.busy": "2024-05-19T11:31:39.637932Z",
     "iopub.status.idle": "2024-05-19T11:32:01.430520Z",
     "shell.execute_reply": "2024-05-19T11:32:01.429087Z",
     "shell.execute_reply.started": "2024-05-19T11:31:39.638914Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.4.2\n",
      "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.2) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.2) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.2) (3.2.0)\n",
      "Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import gc\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, PowerTransformer, OrdinalEncoder\n",
    ")\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedGroupKFold\n",
    ")\n",
    "from lightgbm import (\n",
    "    LGBMClassifier, early_stopping, log_evaluation, plot_importance,\n",
    ")\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "from xgboost.callback import EarlyStopping\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df):\n",
    "    \"\"\"\n",
    "    Reduce memory usage of a Pandas DataFrame by converting \n",
    "    object types to categories and downcasting numeric columns\n",
    "    \"\"\"\n",
    "    # Column types\n",
    "    object_cols, int_cols, float_cols = [], [], []\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        if pd.api.types.is_object_dtype(dtype):\n",
    "            object_cols.append(col)\n",
    "        elif pd.api.types.is_integer_dtype(dtype):\n",
    "            int_cols.append(col)\n",
    "        elif pd.api.types.is_float_dtype(dtype):\n",
    "            float_cols.append(col)\n",
    "        \n",
    "    # Convert object columns to category\n",
    "    df[object_cols] = df[object_cols].astype('category')\n",
    "\n",
    "    # Downcast integer columns\n",
    "    df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='integer')\n",
    "   \n",
    "    # Downcast float columns\n",
    "    df[float_cols] = df[float_cols].apply(pd.to_numeric, downcast='float')\n",
    "        \n",
    "    return df\n",
    "\n",
    "def cols_types(df):\n",
    "    \"\"\"\n",
    "    Create lists of feature names dtype\n",
    "    \"\"\"\n",
    "    date_cols, num_cols, cat_cols = [], [], []\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        if pd.api.types.is_bool_dtype(dtype):\n",
    "            cat_cols.append(col)\n",
    "        elif pd.api.types.is_datetime64_dtype(dtype):\n",
    "            date_cols.append(col)\n",
    "        elif pd.api.types.is_numeric_dtype(dtype):\n",
    "            num_cols.append(col)\n",
    "        else:\n",
    "            cat_cols.append(col)\n",
    "            \n",
    "    return date_cols, num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polars functions to read and preprocess data\n",
    "Many aggregation functions are commented out, but available for any experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_cols_types(df):\n",
    "    \"\"\"\n",
    "    (Polars version)\n",
    "    Create lists of feature names dtype\n",
    "    \"\"\"\n",
    "    date_cols, num_cols, cat_cols = [], [], []\n",
    "    \n",
    "    num_cols = df.select(pl.col(pl.NUMERIC_DTYPES)).columns\n",
    "    date_cols = df.select(pl.col(pl.Date)).columns\n",
    "    cat_cols = [col for col in df.columns \n",
    "                if col not in num_cols and col not in date_cols]\n",
    "            \n",
    "    return date_cols, num_cols, cat_cols\n",
    "\n",
    "def aggregate_depth1(df):\n",
    "    \"\"\"\n",
    "    (Polars version)\n",
    "    Aggregate depth=1 dataframe and return a depth=0 dataframe \n",
    "    \"\"\"\n",
    "    # Drop 'num_group1' column\n",
    "    df = df.drop('num_group1')\n",
    "\n",
    "    # Create aggregation dataframe and count repetitive case_id\n",
    "    col_to_count = df.columns[1]\n",
    "    all_agg = df.groupby('case_id').agg(\n",
    "        pl.col(col_to_count).count().alias(f'{col_to_count}_count')\n",
    "    )\n",
    "    # Columns types \n",
    "    date_cols, num_cols, cat_cols = pl_cols_types(df)\n",
    "    num_cols.remove('case_id')\n",
    "\n",
    "    # Aggregate categorical columns\n",
    "    if len(cat_cols) > 0:\n",
    "        for col in cat_cols:\n",
    "            if not df[col].is_null().all():\n",
    "                cat_agg = df.groupby('case_id').agg(\n",
    "                    pl.col(col).drop_nulls().mode().first().alias(f'{col}_mode'),\n",
    "#                     pl.n_unique(col).alias(f'{col}_n_unique'),\n",
    "#                     pl.col(col).first().alias(f'{col}_first'),\n",
    "#                     pl.col(col).drop_nulls().last().alias(f'{col}_last'),\n",
    "                )\n",
    "                # Drop aggregated column to free memory\n",
    "                df = df.drop(col)\n",
    "\n",
    "                # Merge with aggregated dataframe\n",
    "                all_agg = all_agg.join(cat_agg, on='case_id', how='left')\n",
    "\n",
    "                # Free memory\n",
    "                del cat_agg\n",
    "\n",
    "    # Aggregate date columns\n",
    "    if len(date_cols) > 0:\n",
    "        for col in date_cols:\n",
    "            date_agg = df.groupby('case_id').agg(\n",
    "                pl.mean(col).alias(f'{col}_mean'), \n",
    "#                 pl.col(col).first().alias(f'{col}_first'),\n",
    "#                 pl.col(col).drop_nulls().last().alias(f'{col}_last'),\n",
    "            )\n",
    "            # Drop aggregated column to free memory\n",
    "            df = df.drop(col)\n",
    "\n",
    "            # Merge with aggregated dataframe\n",
    "            all_agg = all_agg.join(date_agg, on='case_id', how='left')\n",
    "\n",
    "            # Free memory\n",
    "            del date_agg\n",
    "\n",
    "    # Aggregate numeric columns \n",
    "    if len(num_cols) > 0:\n",
    "        for col in num_cols:\n",
    "            num_agg = df.groupby('case_id').agg(\n",
    "                pl.mean(col).alias(f'{col}_mean'), \n",
    "#                 pl.median(col).alias(f'{col}_median'), \n",
    "#                 pl.min(col).alias(f'{col}_min'), \n",
    "#                 pl.max(col).alias(f'{col}_max'), \n",
    "#                 pl.col(col).first().alias(f'{col}_first'),\n",
    "#                 pl.col(col).drop_nulls().last().alias(f'{col}_last'),\n",
    "            )\n",
    "            # Drop aggregated column to free memory\n",
    "            df = df.drop(col)\n",
    "\n",
    "            # Merge with aggregated dataframe\n",
    "            all_agg = all_agg.join(num_agg, on='case_id', how='left')\n",
    "\n",
    "            # Free memory\n",
    "            del num_agg\n",
    "  \n",
    "    print('Depth1 aggregation finished')    \n",
    "    return all_agg\n",
    "\n",
    "def aggregate_depth2(df):\n",
    "    \"\"\"\n",
    "    (Polars version)\n",
    "    Aggregate depth=2 dataframe to level depth=1 and then apply \n",
    "    aggregate_depth1 function to return a depth=0 dataframe\n",
    "    \"\"\"\n",
    "    df = df.drop('num_group2')\n",
    "    \n",
    "    # Columns types\n",
    "    groupby_cols = ['case_id', 'num_group1']\n",
    "    date_cols, num_cols, cat_cols = pl_cols_types(df)\n",
    "    num_cols = [col for col in num_cols if col not in groupby_cols]\n",
    "    \n",
    "    # Create aggregation dataframe\n",
    "    all_agg = df[groupby_cols]\n",
    "    all_agg = all_agg.unique(groupby_cols, maintain_order=True)\n",
    "    \n",
    "    # Aggregate categoricals \n",
    "    if len(cat_cols) > 0:\n",
    "        for col in cat_cols:\n",
    "            if not df[col].is_null().all():\n",
    "                cat_agg = df.groupby('case_id').agg(\n",
    "                    pl.col(col).drop_nulls().mode().first().alias(f'{col}_mode'),\n",
    "#                     pl.col(col).drop_nulls().first().alias(f'{col}_first'),\n",
    "                )\n",
    "                # Drop aggregated column to free memory\n",
    "                df = df.drop(col)\n",
    "\n",
    "                # Merge with aggregated dataframe\n",
    "                all_agg = all_agg.join(cat_agg, on='case_id', how='left')\n",
    "\n",
    "                # Free memory\n",
    "                del cat_agg\n",
    "    \n",
    "    # Aggregate date columns\n",
    "    if len(date_cols) > 0:\n",
    "        for col in date_cols:\n",
    "            date_agg = df.groupby('case_id').agg(\n",
    "                pl.mean(col).alias(f'{col}_mean'), \n",
    "#                 pl.col(col).drop_nulls().first().alias(f'{col}_first'),\n",
    "            )\n",
    "            # Drop aggregated column to free memory\n",
    "            df = df.drop(col)\n",
    "\n",
    "            # Merge with aggregated dataframe\n",
    "            all_agg = all_agg.join(date_agg, on='case_id', how='left')\n",
    "\n",
    "            # Free memory\n",
    "            del date_agg\n",
    "    \n",
    "    # Aggregate numeric columns if any\n",
    "    if len(num_cols) > 0:\n",
    "        for col in num_cols:\n",
    "            num_agg = df.groupby('case_id').agg(\n",
    "                pl.mean(col).alias(f'{col}_mean'), \n",
    "#                 pl.median(col).alias(f'{col}_median'),\n",
    "#                 pl.col(col).drop_nulls().first().alias(f'{col}_first'),\n",
    "            )\n",
    "            # Drop aggregated column to free memory\n",
    "            df = df.drop(col)\n",
    "\n",
    "            # Merge with aggregated dataframe\n",
    "            all_agg = all_agg.join(num_agg, on='case_id', how='left')\n",
    "\n",
    "            # Free memory\n",
    "            del num_agg\n",
    " \n",
    "    del df\n",
    "    print('Depth2 aggregation finished') \n",
    "    return aggregate_depth1(all_agg)\n",
    "\n",
    "def create_df_from(path, file_name, depth=0):\n",
    "    \"\"\"\n",
    "    (Polars version)\n",
    "    Preprocess files in chunks \n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for i, file_path in enumerate(\n",
    "        glob.glob(path + '*' + file_name + '*.parquet')\n",
    "    ):\n",
    "        df = pl.read_parquet(file_path)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if (col[-1] == 'D') or (col == 'date_decision'):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col in ['case_id', 'WEEK_NUM', 'num_group1', 'num_group2']:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "            elif 'person' in col:\n",
    "                df = df.with_columns(\n",
    "                    pl.col(col).cast(pl.String).cast(pl.Categorical))\n",
    "            elif 'month' in col and 'T' in col:\n",
    "                df = df.with_columns(\n",
    "                    pl.col(col).cast(pl.String).cast(pl.Categorical))\n",
    "        \n",
    "        if depth == 2:\n",
    "            df = aggregate_depth2(df)\n",
    "            \n",
    "        elif depth == 1:\n",
    "            df = aggregate_depth1(df) \n",
    "\n",
    "        dfs.append(df)\n",
    "        print(f'Chunk {i} added to list')\n",
    "    \n",
    "    return pl.concat(dfs, how='diagonal_relaxed')\n",
    "\n",
    "def read_prepare_all(path, files_dict):\n",
    "    \"\"\"\n",
    "    (Polars version)\n",
    "    Read, preprocess and merge all the files together\n",
    "    Return a pandas dataframe\n",
    "    \"\"\"\n",
    "    # Read base data frame\n",
    "    df_all = create_df_from(path, 'base')\n",
    "    print(f'base created')\n",
    "\n",
    "    # Read and aggregate \n",
    "    for depth, files_list in files_dict.items():\n",
    "        for file in files_list:\n",
    "            # Create dataframe from file chunks\n",
    "            print(f'### Start read {file}')\n",
    "            df = create_df_from(path, file, depth)\n",
    "            \n",
    "            # Join with the main dataframe\n",
    "            df_all = df_all.join(df, how='left', on='case_id')\n",
    "            print(f'=== {file} merged to df_all')\n",
    "            \n",
    "            # Convert to Categorical to free memory\n",
    "            df_all = df_all.with_columns(\n",
    "                pl.col(pl.String).cast(pl.Categorical))\n",
    "            df_all = df_all.with_columns(\n",
    "                pl.col(pl.Float64).cast(pl.Float32))\n",
    "        \n",
    "    # Free memory\n",
    "    del df\n",
    "    \n",
    "    # Columns types\n",
    "    date_cols, num_cols, cat_cols = pl_cols_types(df_all)\n",
    "    \n",
    "    # Convert to pandas in chunks to not explode memory use\n",
    "    df_pd = df_all.select(pl.col(num_cols)).to_pandas()\n",
    "    df_all = df_all.drop(num_cols)\n",
    "    df_pd = df_pd.join(df_all.select(pl.col(date_cols)).to_pandas())\n",
    "    df_all = df_all.drop(date_cols)\n",
    "    df_pd = df_pd.join(df_all.select(pl.col(cat_cols)).to_pandas())\n",
    "    del df_all\n",
    "    print('df converted to pandas')\n",
    "    \n",
    "    # Create time features\n",
    "    df_pd['birth_year'] = df_pd.birth_259D_mean.dt.year\n",
    "    df_pd['decision_year'] = df_pd.date_decision.dt.year\n",
    "    df_pd['decision_quarter'] = (\n",
    "        df_pd.date_decision.dt.quarter.astype(str).astype('category'))\n",
    "    df_pd['decision_month_of_year'] = (\n",
    "        df_pd.date_decision.dt.month.astype(str).astype('category'))\n",
    "    df_pd['decision_day_of_month'] = df_pd.date_decision.dt.day\n",
    "    df_pd['decision_day_of_year'] = df_pd.date_decision.dt.dayofyear\n",
    "    df_pd['decision_week_of_year'] = df_pd.date_decision.dt.isocalendar().week\n",
    "    df_pd['decision_day_of_week'] = (\n",
    "        (df_pd.date_decision.dt.dayofweek + 1).astype(str).astype('category'))\n",
    "    \n",
    "    return downcast(df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T22:44:38.004292Z",
     "iopub.status.busy": "2024-05-18T22:44:38.003518Z",
     "iopub.status.idle": "2024-05-18T22:44:38.030169Z",
     "shell.execute_reply": "2024-05-18T22:44:38.029057Z",
     "shell.execute_reply.started": "2024-05-18T22:44:38.004256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "main_path = '/kaggle/input/home-credit-credit-risk-model-stability/'\n",
    "train_path = main_path + 'parquet_files/train/'\n",
    "test_path = main_path + 'parquet_files/test/'\n",
    "\n",
    "# Read info files\n",
    "feat_def = pd.read_csv(main_path + 'feature_definitions.csv')\n",
    "submit = pd.read_csv(main_path + 'sample_submission.csv')\n",
    "\n",
    "# Lists of file names\n",
    "files_dict = {\n",
    "    0: ['static_0', 'static_cb_0'],\n",
    "    1: ['credit_bureau_a_1', 'credit_bureau_b_1', 'applprev_1', \n",
    "        'debitcard_1', 'deposit_1', 'other_1', 'person_1', \n",
    "        'tax_registry_a_1', 'tax_registry_b_1', 'tax_registry_c_1'],\n",
    "    2: ['credit_bureau_a_2', 'credit_bureau_b_2', 'applprev_2', 'person_2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T22:44:38.756139Z",
     "iopub.status.busy": "2024-05-18T22:44:38.755534Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 added to list\n",
      "base created\n",
      "### Start read static_0\n",
      "Chunk 0 added to list\n",
      "Chunk 1 added to list\n",
      "=== static_0 merged to df_all\n",
      "### Start read static_cb_0\n",
      "Chunk 0 added to list\n",
      "=== static_cb_0 merged to df_all\n",
      "### Start read credit_bureau_a_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "Depth1 aggregation finished\n",
      "Chunk 1 added to list\n",
      "Depth1 aggregation finished\n",
      "Chunk 2 added to list\n",
      "Depth1 aggregation finished\n",
      "Chunk 3 added to list\n",
      "=== credit_bureau_a_1 merged to df_all\n",
      "### Start read credit_bureau_b_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "=== credit_bureau_b_1 merged to df_all\n",
      "### Start read applprev_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "Depth1 aggregation finished\n",
      "Chunk 1 added to list\n",
      "=== applprev_1 merged to df_all\n",
      "### Start read debitcard_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "=== debitcard_1 merged to df_all\n",
      "### Start read deposit_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "=== deposit_1 merged to df_all\n",
      "### Start read other_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "=== other_1 merged to df_all\n",
      "### Start read person_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "=== person_1 merged to df_all\n",
      "### Start read tax_registry_a_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "=== tax_registry_a_1 merged to df_all\n",
      "### Start read tax_registry_b_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "=== tax_registry_b_1 merged to df_all\n",
      "### Start read tax_registry_c_1\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "=== tax_registry_c_1 merged to df_all\n",
      "### Start read credit_bureau_a_2\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 0 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 1 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 2 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 3 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 4 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 5 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 6 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 7 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 8 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 9 added to list\n",
      "Depth2 aggregation finished\n",
      "Depth1 aggregation finished\n",
      "Chunk 10 added to list\n",
      "=== credit_bureau_a_2 merged to df_all\n",
      "=== credit_bureau_a_2 merged to df_all\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Process / restore point\n",
    "process = True\n",
    "\n",
    "if process:\n",
    "    # Read, preprocess and merge all the training files together\n",
    "    X = read_prepare_all(train_path, files_dict)\n",
    "    \n",
    "    # Backup processed data\n",
    "    X.to_parquet('X.parquet')\n",
    "    \n",
    "else:\n",
    "    # Restore processed data from backup\n",
    "    X = pd.read_parquet('/kaggle/input/creditrisk-data/X.parquet') \n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for date features\n",
    "def make_time_features(df, date_col):\n",
    "    plot_data = df[['target', 'WEEK_NUM']]\n",
    "    plot_data['day_of_year'] = df[date_col].dt.dayofyear.astype('int16')\n",
    "    plot_data['month_of_year'] = df[date_col].dt.month.astype('int8')\n",
    "    plot_data['day_of_month'] = df[date_col].dt.day.astype('int8')\n",
    "    plot_data['day_of_week'] = (df[date_col].dt.dayofweek + 1).astype('int8')\n",
    "    return plot_data\n",
    "\n",
    "# Create time related features for analysis\n",
    "plot_data = make_time_features(X, 'date_decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "plt.figure(figsize=(5, 1))\n",
    "plt.title('Target distribution')\n",
    "sns.countplot(data=plot_data, y='target')\n",
    "plt.show()\n",
    "print(f'Targets == 1 are: {plot_data.target.mean().round(2) * 100}% from train data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T09:13:13.016534Z",
     "iopub.status.busy": "2024-02-12T09:13:13.016101Z",
     "iopub.status.idle": "2024-02-12T09:13:13.028114Z",
     "shell.execute_reply": "2024-02-12T09:13:13.0268Z",
     "shell.execute_reply.started": "2024-02-12T09:13:13.016502Z"
    }
   },
   "source": [
    "* Target is unbalanced (97/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target evolution in time\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.title('Target evolution in time')\n",
    "g = sns.lineplot(data=plot_data, x='WEEK_NUM', y='target')\n",
    "g.set(xticks=np.arange(0, 93, 4))\n",
    "plt.grid() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A big drop of total defaults was registered between weeks 62 - 65 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution by month_of_year\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.title('Target distribution by month of the year')\n",
    "g = sns.lineplot(data=plot_data, x='month_of_year', y='target')\n",
    "g.set(xticks=np.arange(1, 13, 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution by day_of_year\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.title('Target distribution by day of the year')\n",
    "g = sns.lineplot(data=plot_data, x='day_of_year', y='target')\n",
    "g.set(xticks=np.arange(1, 366, 30))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Strange behaviour on day 335, the only day without any target == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution by day_of_month\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.title('Target distribution by day of month')\n",
    "g = sns.lineplot(data=plot_data, x='day_of_month', y='target')\n",
    "g.set(xticks=np.arange(1, 32, 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution by day_of_week\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.title('Target distribution by day of week')\n",
    "g = sns.lineplot(data=plot_data, x='day_of_week', y='target')\n",
    "g.set(xticks=np.arange(1, 8, 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weeks with the highest missing values ratio\n",
    "df_missing = X.groupby('WEEK_NUM').case_id.count().reset_index()\n",
    "df_missing = df_missing.rename(columns={'case_id': 'Count'})\n",
    "\n",
    "for week in X.WEEK_NUM.unique():\n",
    "    missing = X[X.WEEK_NUM.eq(week)].isna().sum().sum()\n",
    "    size = X[X.WEEK_NUM.eq(week)].size\n",
    "    df_missing.loc[df_missing.WEEK_NUM.eq(week), 'Missing_ratio'] = missing / size\n",
    "\n",
    "df_missing.sort_values('Missing_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* week 0 has very high missing ratio comparing to other weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which weeks has the most maximum values by columns\n",
    "max_df = X.select_dtypes(np.number).groupby('WEEK_NUM').max()\n",
    "max_week_list = []\n",
    "for col in max_df.columns:\n",
    "    max_week = max_df.sort_values(col, ascending=False).iloc[0].name\n",
    "    max_week_list.append(max_week)\n",
    "max_week_df = pd.DataFrame(max_week_list)\n",
    "max_week_df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* weeks 0 and 91 have very much maximum values comparing to other weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pipeline transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFeatTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to create time related features\n",
    "    \"\"\"\n",
    "    def fit(self, df, y=None):\n",
    "        self.ref_cols = ['birth_259D_mean', 'date_decision']\n",
    "        self.original_cols = [col for col in df.columns \n",
    "                              if col not in self.ref_cols]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        # Create time delta features\n",
    "        for col in self.original_cols:\n",
    "            delta_col_0 = f'delta_{col}_{self.ref_cols[0]}'\n",
    "            df[delta_col_0] = abs(df[col] - df[self.ref_cols[0]]).dt.days\n",
    "\n",
    "            delta_col_1 = f'delta_{col}_{self.ref_cols[1]}'\n",
    "            df[delta_col_1] = abs(df[col] - df[self.ref_cols[1]]).dt.days\n",
    "            \n",
    "        delta_col_0_1 = f'delta_{self.ref_cols[1]}_{self.ref_cols[0]}'      \n",
    "        df[delta_col_0_1] = abs(df[self.ref_cols[1]] - df[self.ref_cols[0]]).dt.days\n",
    "        \n",
    "        # Drop used cols \n",
    "        df = df.drop(self.ref_cols + self.original_cols, axis=1)\n",
    "        \n",
    "        self.delta_cols = df.columns.to_list()\n",
    "        \n",
    "        return df\n",
    "   \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.delta_cols\n",
    "\n",
    "class NumFeatTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to create numeric related features\n",
    "    \"\"\"\n",
    "    def fit(self, df, y=None):\n",
    "        self.ref_cols = ['birth_year', 'decision_year']\n",
    "        self.year_cols = []\n",
    "        for col in df.columns:\n",
    "            if 'year' in col and 'T' in col:\n",
    "                self.year_cols.append(col)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        # Create year delta features\n",
    "        for col in self.year_cols:\n",
    "            delta_col_0 = f'delta_{col}_{self.ref_cols[0]}'\n",
    "            df[delta_col_0] = abs(df[col] - df[self.ref_cols[0]])\n",
    "\n",
    "            delta_col_1 = f'delta_{col}_{self.ref_cols[1]}'\n",
    "            df[delta_col_1] = abs(df[col] - df[self.ref_cols[1]])\n",
    "\n",
    "        delta_col_0_1 = f'delta_{self.ref_cols[1]}_{self.ref_cols[0]}'   \n",
    "        df[delta_col_0_1] = abs(df[self.ref_cols[1]] - df[self.ref_cols[0]])\n",
    "        \n",
    "        df = df.drop(self.ref_cols + self.year_cols, axis=1)\n",
    "        \n",
    "        self.all_cols = df.columns.to_list()\n",
    "\n",
    "        return df.astype(float)\n",
    "   \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.all_cols\n",
    "\n",
    "class BadColsDropTransformer(BaseEstimator, TransformerMixin): \n",
    "    \"\"\"\n",
    "    Transformer to drop unuseful columns\n",
    "    \"\"\"\n",
    "    def fit(self, df, y=None):\n",
    "        # Columns with many missing values \n",
    "        self.missing_values = df.isna().mean().sort_values(ascending=False)\n",
    "        self.cols_to_drop = set(\n",
    "            self.missing_values[self.missing_values.gt(0.95)].index\n",
    "        )\n",
    "        # Columns with one higly dominant value\n",
    "        for col in df.columns:\n",
    "            if (df[col].value_counts(normalize=True) > 0.95).any():\n",
    "                self.cols_to_drop.add(col)\n",
    "                \n",
    "        # Columns with identical values  \n",
    "        for col1 in df.columns[:-1]:\n",
    "            if col1 not in self.cols_to_drop:\n",
    "                for col2 in df.columns[df.columns.get_loc(col1) + 1:]:\n",
    "                    if df[col1].equals(df[col2]):\n",
    "                        self.cols_to_drop.add(col2)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, df):\n",
    "        return df.drop(list(self.cols_to_drop), axis=1)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [col for col in input_features \n",
    "                if col not in self.cols_to_drop]\n",
    "    \n",
    "class HighCorrDropTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to drop highly correlated numerical columns\n",
    "    \"\"\"\n",
    "    def fit(self, df, y=None):\n",
    "        self.corr_matrix = df.corr()\n",
    "        self.cols_to_drop = set()\n",
    "        for col1 in self.corr_matrix.columns:\n",
    "            for col2 in self.corr_matrix.columns:\n",
    "                if col1 != col2:\n",
    "                    # Check for high correlation\n",
    "                    if abs(self.corr_matrix.loc[col1, col2]) >= 0.90:\n",
    "                        # Check which column has more missing values\n",
    "                        if df[col1].isna().sum() > df[col2].isna().sum():\n",
    "                            self.cols_to_drop.add(col1)\n",
    "                        else:\n",
    "                            self.cols_to_drop.add(col2) \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df.drop(list(self.cols_to_drop), axis=1)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [col for col in input_features if col not in self.cols_to_drop]\n",
    "        \n",
    "class LowFreqTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to process categorical, boolean and object columns\n",
    "    Fill missing and convert infrequent values\n",
    "    \"\"\"\n",
    "    def fit(self, df, y=None):\n",
    "        self.original_cols = df.columns\n",
    "        self.frequencies = {}\n",
    "        self.threshold = {}\n",
    "        for col in df.columns:\n",
    "            self.frequencies[col] = df[col].value_counts(normalize=True, \n",
    "                                                         ascending=True)\n",
    "            self.threshold[col] = self.frequencies[col][\n",
    "                (self.frequencies[col].cumsum() > 0.05).idxmax()\n",
    "                ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        for col in self.original_cols:\n",
    "            df[col] = df[col].astype(str)\n",
    "            \n",
    "            infrequent_mask = (df[col].isin(\n",
    "                self.frequencies[col].index[\n",
    "                    self.frequencies[col] < self.threshold[col]\n",
    "                ]))\n",
    "            # Convert low frequency categoricals to 'infrequent'\n",
    "            df.loc[infrequent_mask, col] = 'infrequent'\n",
    "        return df\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return input_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outlier weeks after analysis\n",
    "#X = X[~X.WEEK_NUM.eq(0)]\n",
    "\n",
    "# Drop day 335 as an outlier, the only date without any target=1\n",
    "#X = X[~X['date_decision'].dt.dayofyear.eq(335)]\n",
    "\n",
    "# Separate target\n",
    "y = X.pop('target')\n",
    "\n",
    "# Keep a copy of WEEK_NUM for group splitting in modeling\n",
    "week_num = X.WEEK_NUM\n",
    "\n",
    "# Drop unuseful and duplicate columns\n",
    "cols_to_drop = [\n",
    "    'case_id', 'MONTH', 'WEEK_NUM', 'birthdate_574D', 'dateofbirth_337D',  \n",
    "]\n",
    "X = X.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Keep the structure of X to match it later with X_test \n",
    "X_structure = X.drop(X.index)\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Separate columns by type\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m date_cols, num_cols, cat_cols \u001b[38;5;241m=\u001b[39m cols_types(\u001b[43mX\u001b[49m)\n\u001b[0;32m      3\u001b[0m cat_unique \u001b[38;5;241m=\u001b[39m X[cat_cols]\u001b[38;5;241m.\u001b[39mnunique()\n\u001b[0;32m      4\u001b[0m low_card_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(cat_unique\u001b[38;5;241m.\u001b[39mindex[cat_unique\u001b[38;5;241m.\u001b[39mle(\u001b[38;5;241m12\u001b[39m)])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Separate columns by type\n",
    "date_cols, num_cols, cat_cols = cols_types(X)\n",
    "cat_unique = X[cat_cols].nunique()\n",
    "low_card_cols = list(cat_unique.index[cat_unique.le(12)])\n",
    "med_card_cols = list(cat_unique.index[cat_unique.gt(12) & cat_unique.le(200)])\n",
    "high_card_cols = list(cat_unique.index[cat_unique.gt(200)])\n",
    "\n",
    "# Pipeline to process date columns\n",
    "date_pipeline = make_pipeline(\n",
    "    TimeFeatTransformer(),\n",
    "    BadColsDropTransformer(),\n",
    "    HighCorrDropTransformer(),\n",
    "    PowerTransformer(copy=False),\n",
    "    )\n",
    "# Pipeline to process numerical columns\n",
    "num_pipeline = make_pipeline(\n",
    "    NumFeatTransformer(),\n",
    "    BadColsDropTransformer(),\n",
    "    HighCorrDropTransformer(),\n",
    "    PowerTransformer(copy=False),\n",
    "    )\n",
    "# Pipeline to process low cardinality columns\n",
    "low_card_pipeline = make_pipeline(\n",
    "    BadColsDropTransformer(),\n",
    "    LowFreqTransformer(),\n",
    "    OneHotEncoder(\n",
    "        dtype=np.int8, drop='if_binary', sparse_output=False,\n",
    "        min_frequency=0.02, handle_unknown='infrequent_if_exist'),\n",
    "    )\n",
    "# Pipeline to process medium cardinality columns\n",
    "med_card_pipeline = make_pipeline(\n",
    "    BadColsDropTransformer(),\n",
    "    LowFreqTransformer(),\n",
    "    OrdinalEncoder(handle_unknown='use_encoded_value',\n",
    "                   unknown_value=np.nan,\n",
    "                   dtype=np.float32),\n",
    "    )\n",
    "# Pipeline to process high cardinality columns\n",
    "high_card_pipeline = make_pipeline(\n",
    "    BadColsDropTransformer(),\n",
    "    LowFreqTransformer(),\n",
    "    TargetEncoder(target_type='binary', smooth='auto', shuffle=True),\n",
    "    PowerTransformer(copy=False),\n",
    "    )\n",
    "# Define column transformer\n",
    "processor = make_column_transformer(\n",
    "    (date_pipeline, date_cols),\n",
    "    (num_pipeline, num_cols),\n",
    "    (low_card_pipeline, low_card_cols),\n",
    "    (med_card_pipeline, med_card_cols),\n",
    "    (high_card_pipeline, high_card_cols),\n",
    "    verbose=True,\n",
    "    )\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(processor.fit_transform(X, y), \n",
    "                       columns=processor.get_feature_names_out(),\n",
    "                       index=X.index)\n",
    "X = downcast(X)\n",
    "enc_med_card_cols = []\n",
    "for col in med_card_cols:\n",
    "    if 'pipeline-4__' + col in X.columns:\n",
    "        enc_med_card_cols.append('pipeline-4__' + col)\n",
    "\n",
    "X[enc_med_card_cols] = X[enc_med_card_cols].astype('str').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_parquet('/kaggle/working/train_v4.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAM TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data_proc as dp\n",
    "base, X, y = dp.load_data('data/train_v3_filled_woe.parquet')\n",
    "X4 = pd.read_parquet('data/train_v4.parquet')\n",
    "X = X.merge(X4, left_index=True, right_index=True, how='left').astype(float)\n",
    "\n",
    "del X4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1221229 entries, 157793 to 658664\n",
      "Columns: 969 entries, month_decision to pipeline-5__employername_160M_mode\n",
      "dtypes: float64(969)\n",
      "memory usage: 8.8 GB\n",
      "CPU times: total: 13.6 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#Train / validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, stratify=y)\n",
    "del X, y\n",
    "\n",
    "# Process data\n",
    "\"\"\"X_train = pd.DataFrame(processor.fit_transform(X_train, y_train), \n",
    "                       columns=processor.get_feature_names_out(),\n",
    "                       index=X_train.index)\n",
    "X_val = pd.DataFrame(processor.transform(X_val), \n",
    "                     columns=processor.get_feature_names_out(),\n",
    "                     index=X_val.index)   \n",
    "\n",
    "# Free memory\n",
    "X_train = downcast(X_train)\n",
    "X_val = downcast(X_val)  \n",
    "\"\"\"\n",
    "date_cols, num_cols, cat_cols = cols_types(X_train)\n",
    "cat_unique = X_train[cat_cols].nunique()\n",
    "low_card_cols = list(cat_unique.index[cat_unique.le(12)])\n",
    "med_card_cols = list(cat_unique.index[cat_unique.gt(12) & cat_unique.le(200)])\n",
    "high_card_cols = list(cat_unique.index[cat_unique.gt(200)])\n",
    "# Convert med_card_cols to category to pass them unencoded to the model \n",
    "enc_med_card_cols = []\n",
    "for col in med_card_cols:\n",
    "    if 'pipeline-4__' + col in X_train.columns:\n",
    "        enc_med_card_cols.append('pipeline-4__' + col)\n",
    "\n",
    "X_train[enc_med_card_cols] = X_train[enc_med_card_cols].astype('str').astype('category')\n",
    "X_val[enc_med_card_cols] = X_val[enc_med_card_cols].astype('str').astype('category')\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_num = base['WEEK_NUM']\n",
    "class StabilityMetric:\n",
    "    \"\"\"\n",
    "    Stability metric for model optimization during training\n",
    "    \"\"\"\n",
    "    def __init__(self, X_val):\n",
    "        self.X_val = X_val\n",
    "        \n",
    "    def lgbm_stability_metric(self, y_true, y_pred):\n",
    "        gini_in_time = []\n",
    "        weeks_to_score = week_num[self.X_val.index].reset_index(drop=True)\n",
    "        \n",
    "        for week in weeks_to_score.unique():\n",
    "            week_idx = weeks_to_score.eq(week)\n",
    "            gini = np.array(2 * roc_auc_score(y_true[week_idx], y_pred[week_idx]) - 1)\n",
    "            gini_in_time.append(gini)\n",
    "\n",
    "        w_fallingrate = 88.0\n",
    "        w_resstd = -0.5\n",
    "        x = np.arange(len(gini_in_time))\n",
    "        y = np.array(gini_in_time)\n",
    "        a, b = np.polyfit(x, y, 1)\n",
    "        y_hat = a * x + b\n",
    "        residuals = y - y_hat\n",
    "        res_std = np.std(residuals)\n",
    "        avg_gini = np.mean(y)\n",
    "        stability_score = avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "        is_higher_better = True\n",
    "\n",
    "        return 'stability_score', stability_score, is_higher_better\n",
    "\n",
    "    def xgb_stability_metric(self, y_pred, y_true):\n",
    "        if isinstance(y_true, DMatrix):\n",
    "            y_true = y_true.get_label()\n",
    "            \n",
    "        gini_in_time = []\n",
    "        weeks_to_score = week_num[self.X_val.index].reset_index(drop=True)\n",
    "        \n",
    "        for week in weeks_to_score.unique():\n",
    "            week_idx = weeks_to_score.eq(week)\n",
    "            gini = np.array(2 * roc_auc_score(y_true[week_idx], y_pred[week_idx]) - 1)\n",
    "            gini_in_time.append(gini)\n",
    "\n",
    "        w_fallingrate = 88.0\n",
    "        w_resstd = -0.5\n",
    "        x = np.arange(len(gini_in_time))\n",
    "        y = np.array(gini_in_time)\n",
    "        a, b = np.polyfit(x, y, 1)\n",
    "        y_hat = a * x + b\n",
    "        residuals = y - y_hat\n",
    "        res_std = np.std(residuals)\n",
    "        avg_gini = np.mean(y)\n",
    "        stability_score = avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "        return 'stability_score', stability_score\n",
    "\n",
    "def lgbm_objective(trial):\n",
    "    \"\"\"\n",
    "    LGBMClassifier parameters search\n",
    "    \"\"\"\n",
    "    # Target ratio for unbalanced data\n",
    "    y_ratio = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "    params = {\n",
    "        'n_estimators': 5000,\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 300),\n",
    "        'min_child_samples': trial.suggest_int('min_data_in_leaf', 20, 2000),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.005, 0.1),\n",
    "        'reg_alpha': trial.suggest_loguniform('lambda_l1', 1e-8, 10),\n",
    "        'reg_lambda': trial.suggest_loguniform('lambda_l2', 1e-8, 10),\n",
    "        'colsample_bytree': trial.suggest_uniform('feature_fraction', 0.5, 1),\n",
    "        'subsample': trial.suggest_uniform('bagging_fraction', 0.5, 1),\n",
    "        'subsample_freq': trial.suggest_int('bagging_freq', 0, 10),\n",
    "        \n",
    "        'objective': 'binary',\n",
    "        'metric': 'None',\n",
    "        'verbosity': -1,\n",
    "        'scale_pos_weight': y_ratio,\n",
    "        'device': 'gpu',\n",
    "        'max_bin': 255,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "    custom_metric = StabilityMetric(X_val)\n",
    "\n",
    "    model = LGBMClassifier(**params)  \n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric=custom_metric.lgbm_stability_metric,\n",
    "              callbacks=[log_evaluation(100), early_stopping(100)],\n",
    "              )\n",
    "    y_pred = model.predict_proba(X_val)[:, 1]\n",
    "    _, stability_score, _ = custom_metric.lgbm_stability_metric(\n",
    "        np.array(y_val), np.array(y_pred)\n",
    "    )\n",
    "    return stability_score\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    \"\"\"\n",
    "    XGBClassifier parameters search\n",
    "    \"\"\"\n",
    "    # Target ratio for unbalanced data\n",
    "    y_ratio = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "    params = {\n",
    "        'n_estimators': 5000,\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.005, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 20),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 3000),\n",
    "        'max_delta_setp': trial.suggest_int('max_delta_setp', 0, 10),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
    "        'lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10),\n",
    "        'alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 10),\n",
    "        \n",
    "        'objective': 'binary:logistic',\n",
    "#         'eval_metric': 'auc',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'enable_categorical': True,\n",
    "        'verbosity': 1, \n",
    "        'scale_pos_weight': y_ratio,\n",
    "        'device': 'cuda',\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "    es = EarlyStopping(rounds=100, \n",
    "                       maximize=True,\n",
    "                       save_best=True,\n",
    "                      )\n",
    "    custom_metric = StabilityMetric(X_val)\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric=custom_metric.xgb_stability_metric,\n",
    "              callbacks=[es],\n",
    "             )\n",
    "    y_pred = model.predict_proba(X_val)[:, 1]\n",
    "    _, stability_score = custom_metric.xgb_stability_metric(\n",
    "        np.array(y_pred), np.array(y_val)\n",
    "    )\n",
    "    return stability_score\n",
    "\n",
    "def cb_objective(trial):\n",
    "    \"\"\"\n",
    "    CatBoostClassifier parameters search\n",
    "    \"\"\"\n",
    "    # Target ratio for unbalanced data\n",
    "    y_ratio = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "    params = {\n",
    "        'iterations': 5000,\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.005, 0.1),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-8, 100),\n",
    "        'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0.0, 10),\n",
    "        'random_strength': trial.suggest_loguniform('random_strength', 1e-8, 10),\n",
    "        'depth': trial.suggest_int('depth', 4, 16),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        \n",
    "        'scale_pos_weight': y_ratio,\n",
    "        'objective': 'Logloss',\n",
    "        'eval_metric': 'Logloss', # auc and custom functions are not working on gpu\n",
    "        'verbose': 50, \n",
    "        'task_type': 'GPU',\n",
    "        'thread_count': -1,\n",
    "    }\n",
    "    train_data = Pool(data=X_train, \n",
    "                      label=y_train, \n",
    "                      cat_features=enc_med_card_cols,\n",
    "                      )\n",
    "    val_data = Pool(data=X_val, \n",
    "                    label=y_val, \n",
    "                    cat_features=enc_med_card_cols,\n",
    "                    )    \n",
    "    model = CatBoostClassifier(**params)  \n",
    "    model.fit(train_data,\n",
    "              eval_set=val_data,\n",
    "              early_stopping_rounds=100,\n",
    "              )   \n",
    "    y_pred = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    return roc_auc_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 23:36:37,115] A new study created in memory with name: no-name-5f0e8dad-6cce-43d3-8d6a-27544142aa83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.667382\n",
      "[200]\tvalid_0's stability_score: 0.673653\n",
      "[300]\tvalid_0's stability_score: 0.673731\n",
      "[400]\tvalid_0's stability_score: 0.672637\n",
      "Early stopping, best iteration is:\n",
      "[303]\tvalid_0's stability_score: 0.674164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 23:39:42,327] Trial 0 finished with value: 0.6741638619108079 and parameters: {'num_leaves': 85, 'min_data_in_leaf': 1509, 'learning_rate': 0.06893725206124518, 'lambda_l1': 0.03934534060838883, 'lambda_l2': 1.0654041584742898e-06, 'feature_fraction': 0.646291688350873, 'bagging_fraction': 0.5030451802618435, 'bagging_freq': 7}. Best is trial 0 with value: 0.6741638619108079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.663352\n",
      "[200]\tvalid_0's stability_score: 0.66897\n",
      "[300]\tvalid_0's stability_score: 0.668483\n",
      "Early stopping, best iteration is:\n",
      "[252]\tvalid_0's stability_score: 0.670039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 23:42:23,950] Trial 1 finished with value: 0.6700394944847379 and parameters: {'num_leaves': 63, 'min_data_in_leaf': 199, 'learning_rate': 0.09846487264604088, 'lambda_l1': 0.9481996528578365, 'lambda_l2': 3.9781014295706225e-06, 'feature_fraction': 0.5806800636275802, 'bagging_fraction': 0.6415633421693626, 'bagging_freq': 3}. Best is trial 0 with value: 0.6741638619108079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.664416\n",
      "[200]\tvalid_0's stability_score: 0.673758\n",
      "[300]\tvalid_0's stability_score: 0.678775\n",
      "[400]\tvalid_0's stability_score: 0.679488\n",
      "Early stopping, best iteration is:\n",
      "[348]\tvalid_0's stability_score: 0.680051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 23:46:31,923] Trial 2 finished with value: 0.6800507264111265 and parameters: {'num_leaves': 134, 'min_data_in_leaf': 1651, 'learning_rate': 0.05820191335984723, 'lambda_l1': 2.274365396073248e-08, 'lambda_l2': 7.432069631290359, 'feature_fraction': 0.938662897502058, 'bagging_fraction': 0.8183782639490413, 'bagging_freq': 5}. Best is trial 2 with value: 0.6800507264111265.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.656709\n",
      "[200]\tvalid_0's stability_score: 0.672128\n",
      "[300]\tvalid_0's stability_score: 0.678303\n",
      "[400]\tvalid_0's stability_score: 0.680437\n",
      "[500]\tvalid_0's stability_score: 0.680775\n",
      "[600]\tvalid_0's stability_score: 0.680718\n",
      "[700]\tvalid_0's stability_score: 0.680785\n",
      "[800]\tvalid_0's stability_score: 0.681711\n",
      "Early stopping, best iteration is:\n",
      "[759]\tvalid_0's stability_score: 0.681857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 23:52:17,191] Trial 3 finished with value: 0.6818573122901658 and parameters: {'num_leaves': 54, 'min_data_in_leaf': 433, 'learning_rate': 0.049213655604152265, 'lambda_l1': 2.638393420147147, 'lambda_l2': 3.9290503858941405e-05, 'feature_fraction': 0.9094345170837402, 'bagging_fraction': 0.7189961556188009, 'bagging_freq': 8}. Best is trial 3 with value: 0.6818573122901658.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.663975\n",
      "[200]\tvalid_0's stability_score: 0.670428\n",
      "[300]\tvalid_0's stability_score: 0.672454\n",
      "[400]\tvalid_0's stability_score: 0.6727\n",
      "[500]\tvalid_0's stability_score: 0.67358\n",
      "Early stopping, best iteration is:\n",
      "[484]\tvalid_0's stability_score: 0.674405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 23:58:19,355] Trial 4 finished with value: 0.6744048029105112 and parameters: {'num_leaves': 194, 'min_data_in_leaf': 1746, 'learning_rate': 0.06713693832103466, 'lambda_l1': 0.003753780333883439, 'lambda_l2': 1.6062406972291433e-06, 'feature_fraction': 0.9650385712655265, 'bagging_fraction': 0.7905960243193841, 'bagging_freq': 1}. Best is trial 3 with value: 0.6818573122901658.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.656513\n",
      "[200]\tvalid_0's stability_score: 0.665094\n",
      "[300]\tvalid_0's stability_score: 0.664979\n",
      "Early stopping, best iteration is:\n",
      "[259]\tvalid_0's stability_score: 0.665986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 00:02:03,710] Trial 5 finished with value: 0.6659857525946383 and parameters: {'num_leaves': 254, 'min_data_in_leaf': 246, 'learning_rate': 0.051079624779120494, 'lambda_l1': 1.060832363851471e-07, 'lambda_l2': 1.135559954855427e-05, 'feature_fraction': 0.5446837652424679, 'bagging_fraction': 0.5063830428783496, 'bagging_freq': 8}. Best is trial 3 with value: 0.6818573122901658.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.632358\n",
      "[200]\tvalid_0's stability_score: 0.640417\n",
      "[300]\tvalid_0's stability_score: 0.646238\n",
      "[400]\tvalid_0's stability_score: 0.65254\n",
      "[500]\tvalid_0's stability_score: 0.658666\n",
      "[600]\tvalid_0's stability_score: 0.663697\n",
      "[700]\tvalid_0's stability_score: 0.667829\n",
      "[800]\tvalid_0's stability_score: 0.671147\n",
      "[900]\tvalid_0's stability_score: 0.673834\n",
      "[1000]\tvalid_0's stability_score: 0.67634\n",
      "[1100]\tvalid_0's stability_score: 0.678222\n",
      "[1200]\tvalid_0's stability_score: 0.679106\n",
      "[1300]\tvalid_0's stability_score: 0.680601\n",
      "[1400]\tvalid_0's stability_score: 0.681768\n",
      "[1500]\tvalid_0's stability_score: 0.682683\n",
      "[1600]\tvalid_0's stability_score: 0.683007\n",
      "[1700]\tvalid_0's stability_score: 0.6838\n",
      "[1800]\tvalid_0's stability_score: 0.684438\n",
      "[1900]\tvalid_0's stability_score: 0.685343\n",
      "[2000]\tvalid_0's stability_score: 0.685656\n",
      "[2100]\tvalid_0's stability_score: 0.685951\n",
      "[2200]\tvalid_0's stability_score: 0.68623\n",
      "[2300]\tvalid_0's stability_score: 0.686338\n",
      "Early stopping, best iteration is:\n",
      "[2250]\tvalid_0's stability_score: 0.686412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 00:19:50,440] Trial 6 finished with value: 0.6864117589786688 and parameters: {'num_leaves': 88, 'min_data_in_leaf': 1398, 'learning_rate': 0.007527971787524835, 'lambda_l1': 2.5902094144625828e-05, 'lambda_l2': 2.4865532359136905e-07, 'feature_fraction': 0.617023068364577, 'bagging_fraction': 0.8869282625503894, 'bagging_freq': 6}. Best is trial 6 with value: 0.6864117589786688.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.665018\n",
      "[200]\tvalid_0's stability_score: 0.676899\n",
      "[300]\tvalid_0's stability_score: 0.678953\n",
      "[400]\tvalid_0's stability_score: 0.680454\n",
      "Early stopping, best iteration is:\n",
      "[383]\tvalid_0's stability_score: 0.680928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 00:24:16,342] Trial 7 finished with value: 0.6809276961201459 and parameters: {'num_leaves': 147, 'min_data_in_leaf': 1947, 'learning_rate': 0.049627949283823476, 'lambda_l1': 8.318181157521204e-05, 'lambda_l2': 0.004581246312273876, 'feature_fraction': 0.6299965918565626, 'bagging_fraction': 0.5697193612234792, 'bagging_freq': 5}. Best is trial 6 with value: 0.6864117589786688.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.631773\n",
      "[200]\tvalid_0's stability_score: 0.642116\n",
      "[300]\tvalid_0's stability_score: 0.651709\n",
      "[400]\tvalid_0's stability_score: 0.660318\n",
      "[500]\tvalid_0's stability_score: 0.666318\n",
      "[600]\tvalid_0's stability_score: 0.670656\n",
      "[700]\tvalid_0's stability_score: 0.674037\n",
      "[800]\tvalid_0's stability_score: 0.676445\n",
      "[900]\tvalid_0's stability_score: 0.678801\n",
      "[1000]\tvalid_0's stability_score: 0.680878\n",
      "[1100]\tvalid_0's stability_score: 0.681612\n",
      "[1200]\tvalid_0's stability_score: 0.682256\n",
      "[1300]\tvalid_0's stability_score: 0.682837\n",
      "[1400]\tvalid_0's stability_score: 0.683556\n",
      "[1500]\tvalid_0's stability_score: 0.684856\n",
      "[1600]\tvalid_0's stability_score: 0.68516\n",
      "[1700]\tvalid_0's stability_score: 0.685786\n",
      "[1800]\tvalid_0's stability_score: 0.685936\n",
      "[1900]\tvalid_0's stability_score: 0.686347\n",
      "[2000]\tvalid_0's stability_score: 0.686417\n",
      "[2100]\tvalid_0's stability_score: 0.686662\n",
      "[2200]\tvalid_0's stability_score: 0.686931\n",
      "[2300]\tvalid_0's stability_score: 0.686918\n",
      "Early stopping, best iteration is:\n",
      "[2237]\tvalid_0's stability_score: 0.687195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 00:40:47,375] Trial 8 finished with value: 0.6871954517209767 and parameters: {'num_leaves': 87, 'min_data_in_leaf': 1221, 'learning_rate': 0.009281623487755453, 'lambda_l1': 0.00038939235514527735, 'lambda_l2': 2.6803464736049585e-06, 'feature_fraction': 0.7132340937648285, 'bagging_fraction': 0.5044042539937792, 'bagging_freq': 7}. Best is trial 8 with value: 0.6871954517209767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.638858\n",
      "[200]\tvalid_0's stability_score: 0.650643\n",
      "[300]\tvalid_0's stability_score: 0.662126\n",
      "[400]\tvalid_0's stability_score: 0.669412\n",
      "[500]\tvalid_0's stability_score: 0.673899\n",
      "[600]\tvalid_0's stability_score: 0.677231\n",
      "[700]\tvalid_0's stability_score: 0.679587\n",
      "[800]\tvalid_0's stability_score: 0.681343\n",
      "[900]\tvalid_0's stability_score: 0.682706\n",
      "[1000]\tvalid_0's stability_score: 0.684057\n",
      "[1100]\tvalid_0's stability_score: 0.68517\n",
      "[1200]\tvalid_0's stability_score: 0.685986\n",
      "[1300]\tvalid_0's stability_score: 0.686568\n",
      "[1400]\tvalid_0's stability_score: 0.686445\n",
      "Early stopping, best iteration is:\n",
      "[1312]\tvalid_0's stability_score: 0.686789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 00:53:46,189] Trial 9 finished with value: 0.6867891798519102 and parameters: {'num_leaves': 152, 'min_data_in_leaf': 1454, 'learning_rate': 0.01310319072581281, 'lambda_l1': 5.193513049558476, 'lambda_l2': 0.025609497452307235, 'feature_fraction': 0.730019876586063, 'bagging_fraction': 0.866873253261149, 'bagging_freq': 2}. Best is trial 8 with value: 0.6871954517209767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.607766\n",
      "[200]\tvalid_0's stability_score: 0.63331\n",
      "[300]\tvalid_0's stability_score: 0.646745\n",
      "[400]\tvalid_0's stability_score: 0.654849\n",
      "[500]\tvalid_0's stability_score: 0.66028\n",
      "[600]\tvalid_0's stability_score: 0.664017\n",
      "[700]\tvalid_0's stability_score: 0.667619\n",
      "[800]\tvalid_0's stability_score: 0.670448\n",
      "[900]\tvalid_0's stability_score: 0.672913\n",
      "[1000]\tvalid_0's stability_score: 0.674192\n",
      "[1100]\tvalid_0's stability_score: 0.675188\n",
      "[1200]\tvalid_0's stability_score: 0.675957\n",
      "[1300]\tvalid_0's stability_score: 0.677107\n",
      "[1400]\tvalid_0's stability_score: 0.67789\n",
      "[1500]\tvalid_0's stability_score: 0.67862\n",
      "[1600]\tvalid_0's stability_score: 0.679044\n",
      "[1700]\tvalid_0's stability_score: 0.67927\n",
      "[1800]\tvalid_0's stability_score: 0.679777\n",
      "[1900]\tvalid_0's stability_score: 0.680223\n",
      "[2000]\tvalid_0's stability_score: 0.680479\n",
      "[2100]\tvalid_0's stability_score: 0.680886\n",
      "[2200]\tvalid_0's stability_score: 0.680888\n",
      "Early stopping, best iteration is:\n",
      "[2144]\tvalid_0's stability_score: 0.680945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 01:05:54,416] Trial 10 finished with value: 0.6809446480277598 and parameters: {'num_leaves': 9, 'min_data_in_leaf': 858, 'learning_rate': 0.029240087063338898, 'lambda_l1': 2.9410347401104765e-06, 'lambda_l2': 1.719225089403035e-08, 'feature_fraction': 0.8171730790402636, 'bagging_fraction': 0.9728773253775156, 'bagging_freq': 10}. Best is trial 8 with value: 0.6871954517209767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.631856\n",
      "[200]\tvalid_0's stability_score: 0.639911\n",
      "[300]\tvalid_0's stability_score: 0.645207\n",
      "[400]\tvalid_0's stability_score: 0.650446\n",
      "[500]\tvalid_0's stability_score: 0.654884\n",
      "[600]\tvalid_0's stability_score: 0.659454\n",
      "[700]\tvalid_0's stability_score: 0.663693\n",
      "[800]\tvalid_0's stability_score: 0.667366\n",
      "[900]\tvalid_0's stability_score: 0.670429\n",
      "[1000]\tvalid_0's stability_score: 0.673051\n",
      "[1100]\tvalid_0's stability_score: 0.675191\n",
      "[1200]\tvalid_0's stability_score: 0.676798\n",
      "[1300]\tvalid_0's stability_score: 0.678259\n",
      "[1400]\tvalid_0's stability_score: 0.679709\n",
      "[1500]\tvalid_0's stability_score: 0.680713\n",
      "[1600]\tvalid_0's stability_score: 0.681643\n",
      "[1700]\tvalid_0's stability_score: 0.682643\n",
      "[1800]\tvalid_0's stability_score: 0.683314\n",
      "[1900]\tvalid_0's stability_score: 0.683684\n",
      "[2000]\tvalid_0's stability_score: 0.684153\n",
      "[2100]\tvalid_0's stability_score: 0.684817\n",
      "[2200]\tvalid_0's stability_score: 0.685226\n",
      "[2300]\tvalid_0's stability_score: 0.685405\n",
      "[2400]\tvalid_0's stability_score: 0.685996\n",
      "[2500]\tvalid_0's stability_score: 0.686209\n",
      "[2600]\tvalid_0's stability_score: 0.686497\n",
      "[2700]\tvalid_0's stability_score: 0.68672\n",
      "[2800]\tvalid_0's stability_score: 0.687161\n",
      "[2900]\tvalid_0's stability_score: 0.687428\n",
      "[3000]\tvalid_0's stability_score: 0.687642\n",
      "[3100]\tvalid_0's stability_score: 0.687683\n",
      "[3200]\tvalid_0's stability_score: 0.68804\n",
      "[3300]\tvalid_0's stability_score: 0.688005\n",
      "Early stopping, best iteration is:\n",
      "[3215]\tvalid_0's stability_score: 0.688151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 01:39:40,458] Trial 11 finished with value: 0.6881509914750644 and parameters: {'num_leaves': 204, 'min_data_in_leaf': 1107, 'learning_rate': 0.005322296350733101, 'lambda_l1': 0.01464950362843525, 'lambda_l2': 0.006133498136111812, 'feature_fraction': 0.7482148842739891, 'bagging_fraction': 0.897325244068229, 'bagging_freq': 0}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.652852\n",
      "[200]\tvalid_0's stability_score: 0.670734\n",
      "[300]\tvalid_0's stability_score: 0.678516\n",
      "[400]\tvalid_0's stability_score: 0.682212\n",
      "[500]\tvalid_0's stability_score: 0.683277\n",
      "[600]\tvalid_0's stability_score: 0.684394\n",
      "[700]\tvalid_0's stability_score: 0.684712\n",
      "[800]\tvalid_0's stability_score: 0.684882\n",
      "Early stopping, best iteration is:\n",
      "[779]\tvalid_0's stability_score: 0.685107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 01:49:46,782] Trial 12 finished with value: 0.6851072031434673 and parameters: {'num_leaves': 280, 'min_data_in_leaf': 1049, 'learning_rate': 0.024874099408989835, 'lambda_l1': 0.0028090451066594287, 'lambda_l2': 0.0010761573739551868, 'feature_fraction': 0.7554237569246546, 'bagging_fraction': 0.9827222211667748, 'bagging_freq': 0}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.652874\n",
      "[200]\tvalid_0's stability_score: 0.670588\n",
      "[300]\tvalid_0's stability_score: 0.678303\n",
      "[400]\tvalid_0's stability_score: 0.681142\n",
      "[500]\tvalid_0's stability_score: 0.682555\n",
      "[600]\tvalid_0's stability_score: 0.683449\n",
      "[700]\tvalid_0's stability_score: 0.683719\n",
      "Early stopping, best iteration is:\n",
      "[636]\tvalid_0's stability_score: 0.68402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 01:57:15,107] Trial 13 finished with value: 0.6840200283740652 and parameters: {'num_leaves': 207, 'min_data_in_leaf': 1035, 'learning_rate': 0.027579901938644045, 'lambda_l1': 0.053870769944021395, 'lambda_l2': 0.3114035827802884, 'feature_fraction': 0.7173090065504052, 'bagging_fraction': 0.699894328924575, 'bagging_freq': 3}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.632256\n",
      "[200]\tvalid_0's stability_score: 0.640401\n",
      "[300]\tvalid_0's stability_score: 0.648044\n",
      "[400]\tvalid_0's stability_score: 0.65462\n",
      "[500]\tvalid_0's stability_score: 0.660564\n",
      "[600]\tvalid_0's stability_score: 0.665119\n",
      "[700]\tvalid_0's stability_score: 0.668838\n",
      "[800]\tvalid_0's stability_score: 0.671061\n",
      "[900]\tvalid_0's stability_score: 0.673727\n",
      "[1000]\tvalid_0's stability_score: 0.675583\n",
      "[1100]\tvalid_0's stability_score: 0.676916\n",
      "[1200]\tvalid_0's stability_score: 0.677782\n",
      "[1300]\tvalid_0's stability_score: 0.679238\n",
      "[1400]\tvalid_0's stability_score: 0.679956\n",
      "[1500]\tvalid_0's stability_score: 0.680414\n",
      "[1600]\tvalid_0's stability_score: 0.681323\n",
      "[1700]\tvalid_0's stability_score: 0.681627\n",
      "[1800]\tvalid_0's stability_score: 0.682284\n",
      "[1900]\tvalid_0's stability_score: 0.682549\n",
      "[2000]\tvalid_0's stability_score: 0.682872\n",
      "Early stopping, best iteration is:\n",
      "[1945]\tvalid_0's stability_score: 0.683048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 02:17:45,034] Trial 14 finished with value: 0.6830484885260103 and parameters: {'num_leaves': 213, 'min_data_in_leaf': 711, 'learning_rate': 0.006415675556420283, 'lambda_l1': 0.0006888267182939708, 'lambda_l2': 0.0001229769304326429, 'feature_fraction': 0.8340146148260409, 'bagging_fraction': 0.6245804018174352, 'bagging_freq': 10}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.642949\n",
      "[200]\tvalid_0's stability_score: 0.660754\n",
      "[300]\tvalid_0's stability_score: 0.670942\n",
      "[400]\tvalid_0's stability_score: 0.676704\n",
      "[500]\tvalid_0's stability_score: 0.680579\n",
      "[600]\tvalid_0's stability_score: 0.682949\n",
      "[700]\tvalid_0's stability_score: 0.684459\n",
      "[800]\tvalid_0's stability_score: 0.685368\n",
      "[900]\tvalid_0's stability_score: 0.685808\n",
      "[1000]\tvalid_0's stability_score: 0.686377\n",
      "[1100]\tvalid_0's stability_score: 0.686463\n",
      "Early stopping, best iteration is:\n",
      "[1046]\tvalid_0's stability_score: 0.686804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 02:27:20,542] Trial 15 finished with value: 0.6868041879304275 and parameters: {'num_leaves': 120, 'min_data_in_leaf': 1298, 'learning_rate': 0.01935565583741258, 'lambda_l1': 0.07203418590601807, 'lambda_l2': 0.025192956157022807, 'feature_fraction': 0.8286394463238029, 'bagging_fraction': 0.9116164688911964, 'bagging_freq': 4}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.614973\n",
      "[200]\tvalid_0's stability_score: 0.63946\n",
      "[300]\tvalid_0's stability_score: 0.651427\n",
      "[400]\tvalid_0's stability_score: 0.65907\n",
      "[500]\tvalid_0's stability_score: 0.664443\n",
      "[600]\tvalid_0's stability_score: 0.66869\n",
      "[700]\tvalid_0's stability_score: 0.67217\n",
      "[800]\tvalid_0's stability_score: 0.674039\n",
      "[900]\tvalid_0's stability_score: 0.675785\n",
      "[1000]\tvalid_0's stability_score: 0.676902\n",
      "[1100]\tvalid_0's stability_score: 0.678274\n",
      "[1200]\tvalid_0's stability_score: 0.678739\n",
      "[1300]\tvalid_0's stability_score: 0.679631\n",
      "[1400]\tvalid_0's stability_score: 0.680311\n",
      "[1500]\tvalid_0's stability_score: 0.680791\n",
      "[1600]\tvalid_0's stability_score: 0.681317\n",
      "[1700]\tvalid_0's stability_score: 0.681625\n",
      "[1800]\tvalid_0's stability_score: 0.681971\n",
      "[1900]\tvalid_0's stability_score: 0.68221\n",
      "[2000]\tvalid_0's stability_score: 0.68245\n",
      "[2100]\tvalid_0's stability_score: 0.68261\n",
      "[2200]\tvalid_0's stability_score: 0.683153\n",
      "[2300]\tvalid_0's stability_score: 0.683276\n",
      "[2400]\tvalid_0's stability_score: 0.683644\n",
      "[2500]\tvalid_0's stability_score: 0.684113\n",
      "[2600]\tvalid_0's stability_score: 0.683991\n",
      "Early stopping, best iteration is:\n",
      "[2562]\tvalid_0's stability_score: 0.684137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 02:41:31,816] Trial 16 finished with value: 0.6841371761393052 and parameters: {'num_leaves': 9, 'min_data_in_leaf': 1128, 'learning_rate': 0.03655978641017954, 'lambda_l1': 3.2104327875576224e-06, 'lambda_l2': 3.8742244244167625e-08, 'feature_fraction': 0.7778742978185861, 'bagging_fraction': 0.7805895097779325, 'bagging_freq': 0}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.657915\n",
      "[200]\tvalid_0's stability_score: 0.673653\n",
      "[300]\tvalid_0's stability_score: 0.678829\n",
      "[400]\tvalid_0's stability_score: 0.679601\n",
      "[500]\tvalid_0's stability_score: 0.679743\n",
      "Early stopping, best iteration is:\n",
      "[471]\tvalid_0's stability_score: 0.680966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 02:46:49,734] Trial 17 finished with value: 0.6809656363480742 and parameters: {'num_leaves': 176, 'min_data_in_leaf': 641, 'learning_rate': 0.03878370641770844, 'lambda_l1': 0.00465658698470873, 'lambda_l2': 0.0004614227254377971, 'feature_fraction': 0.6867290090763803, 'bagging_fraction': 0.6531407102621324, 'bagging_freq': 8}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.666553\n",
      "[200]\tvalid_0's stability_score: 0.66793\n",
      "Early stopping, best iteration is:\n",
      "[167]\tvalid_0's stability_score: 0.669924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 02:49:41,388] Trial 18 finished with value: 0.669923588525244 and parameters: {'num_leaves': 243, 'min_data_in_leaf': 1228, 'learning_rate': 0.09568065787011715, 'lambda_l1': 0.00010711592035504625, 'lambda_l2': 0.8560338823071783, 'feature_fraction': 0.5048929080435853, 'bagging_fraction': 0.8399636987119552, 'bagging_freq': 6}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.642193\n",
      "[200]\tvalid_0's stability_score: 0.659022\n",
      "[300]\tvalid_0's stability_score: 0.669763\n",
      "[400]\tvalid_0's stability_score: 0.675575\n",
      "[500]\tvalid_0's stability_score: 0.679997\n",
      "[600]\tvalid_0's stability_score: 0.682436\n",
      "[700]\tvalid_0's stability_score: 0.684149\n",
      "[800]\tvalid_0's stability_score: 0.684794\n",
      "[900]\tvalid_0's stability_score: 0.685883\n",
      "[1000]\tvalid_0's stability_score: 0.686222\n",
      "[1100]\tvalid_0's stability_score: 0.686674\n",
      "[1200]\tvalid_0's stability_score: 0.686633\n",
      "[1300]\tvalid_0's stability_score: 0.686974\n",
      "[1400]\tvalid_0's stability_score: 0.687076\n",
      "[1500]\tvalid_0's stability_score: 0.686948\n",
      "Early stopping, best iteration is:\n",
      "[1419]\tvalid_0's stability_score: 0.687323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 03:01:22,240] Trial 19 finished with value: 0.6873232938255093 and parameters: {'num_leaves': 105, 'min_data_in_leaf': 862, 'learning_rate': 0.01797319199061028, 'lambda_l1': 5.291214484925113e-06, 'lambda_l2': 0.006617328796403234, 'feature_fraction': 0.6752046756824283, 'bagging_fraction': 0.9229435720316811, 'bagging_freq': 2}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.644464\n",
      "[200]\tvalid_0's stability_score: 0.663826\n",
      "[300]\tvalid_0's stability_score: 0.672705\n",
      "[400]\tvalid_0's stability_score: 0.677639\n",
      "[500]\tvalid_0's stability_score: 0.680757\n",
      "[600]\tvalid_0's stability_score: 0.68215\n",
      "[700]\tvalid_0's stability_score: 0.683302\n",
      "[800]\tvalid_0's stability_score: 0.68413\n",
      "[900]\tvalid_0's stability_score: 0.684204\n",
      "Early stopping, best iteration is:\n",
      "[858]\tvalid_0's stability_score: 0.684403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 03:10:35,422] Trial 20 finished with value: 0.6844029066646435 and parameters: {'num_leaves': 172, 'min_data_in_leaf': 841, 'learning_rate': 0.02058904875119336, 'lambda_l1': 7.934442885755701e-07, 'lambda_l2': 0.00860083881382397, 'feature_fraction': 0.8900113280076428, 'bagging_fraction': 0.9373832157005724, 'bagging_freq': 1}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.637495\n",
      "[200]\tvalid_0's stability_score: 0.651704\n",
      "[300]\tvalid_0's stability_score: 0.663266\n",
      "[400]\tvalid_0's stability_score: 0.670736\n",
      "[500]\tvalid_0's stability_score: 0.675317\n",
      "[600]\tvalid_0's stability_score: 0.678747\n",
      "[700]\tvalid_0's stability_score: 0.681762\n",
      "[800]\tvalid_0's stability_score: 0.683404\n",
      "[900]\tvalid_0's stability_score: 0.684429\n",
      "[1000]\tvalid_0's stability_score: 0.685313\n",
      "[1100]\tvalid_0's stability_score: 0.685764\n",
      "[1200]\tvalid_0's stability_score: 0.686164\n",
      "[1300]\tvalid_0's stability_score: 0.686596\n",
      "[1400]\tvalid_0's stability_score: 0.68712\n",
      "[1500]\tvalid_0's stability_score: 0.687199\n",
      "[1600]\tvalid_0's stability_score: 0.687459\n",
      "[1700]\tvalid_0's stability_score: 0.687394\n",
      "Early stopping, best iteration is:\n",
      "[1627]\tvalid_0's stability_score: 0.687651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 03:24:02,436] Trial 21 finished with value: 0.687651035931788 and parameters: {'num_leaves': 105, 'min_data_in_leaf': 875, 'learning_rate': 0.014533682968274977, 'lambda_l1': 2.3796334791986652e-05, 'lambda_l2': 0.17078278275340716, 'feature_fraction': 0.677384761541142, 'bagging_fraction': 0.9394025914188204, 'bagging_freq': 2}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.63903\n",
      "[200]\tvalid_0's stability_score: 0.650684\n",
      "[300]\tvalid_0's stability_score: 0.661726\n",
      "[400]\tvalid_0's stability_score: 0.668315\n",
      "[500]\tvalid_0's stability_score: 0.673466\n",
      "[600]\tvalid_0's stability_score: 0.67718\n",
      "[700]\tvalid_0's stability_score: 0.67909\n",
      "[800]\tvalid_0's stability_score: 0.680963\n",
      "[900]\tvalid_0's stability_score: 0.682474\n",
      "[1000]\tvalid_0's stability_score: 0.683501\n",
      "[1100]\tvalid_0's stability_score: 0.684029\n",
      "[1200]\tvalid_0's stability_score: 0.68442\n",
      "[1300]\tvalid_0's stability_score: 0.685385\n",
      "[1400]\tvalid_0's stability_score: 0.685593\n",
      "[1500]\tvalid_0's stability_score: 0.685828\n",
      "Early stopping, best iteration is:\n",
      "[1468]\tvalid_0's stability_score: 0.686129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 03:36:43,436] Trial 22 finished with value: 0.686129163963065 and parameters: {'num_leaves': 111, 'min_data_in_leaf': 602, 'learning_rate': 0.01378454366413464, 'lambda_l1': 1.8771363967985088e-05, 'lambda_l2': 0.19537042598612422, 'feature_fraction': 0.6777250270575366, 'bagging_fraction': 0.9391941169012458, 'bagging_freq': 2}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.650467\n",
      "[200]\tvalid_0's stability_score: 0.669342\n",
      "[300]\tvalid_0's stability_score: 0.676789\n",
      "[400]\tvalid_0's stability_score: 0.680255\n",
      "[500]\tvalid_0's stability_score: 0.682666\n",
      "[600]\tvalid_0's stability_score: 0.683428\n",
      "[700]\tvalid_0's stability_score: 0.684549\n",
      "[800]\tvalid_0's stability_score: 0.684706\n",
      "[900]\tvalid_0's stability_score: 0.685326\n",
      "[1000]\tvalid_0's stability_score: 0.685858\n",
      "[1100]\tvalid_0's stability_score: 0.685966\n",
      "Early stopping, best iteration is:\n",
      "[1023]\tvalid_0's stability_score: 0.68622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 03:44:13,799] Trial 23 finished with value: 0.6862199166070175 and parameters: {'num_leaves': 53, 'min_data_in_leaf': 879, 'learning_rate': 0.03682969982300936, 'lambda_l1': 1.6943554120821722e-06, 'lambda_l2': 0.08343015304786718, 'feature_fraction': 0.779247060377412, 'bagging_fraction': 0.9957269786376554, 'bagging_freq': 1}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.642539\n",
      "[200]\tvalid_0's stability_score: 0.659211\n",
      "[300]\tvalid_0's stability_score: 0.67027\n",
      "[400]\tvalid_0's stability_score: 0.6759\n",
      "[500]\tvalid_0's stability_score: 0.680243\n",
      "[600]\tvalid_0's stability_score: 0.682785\n",
      "[700]\tvalid_0's stability_score: 0.684132\n",
      "[800]\tvalid_0's stability_score: 0.68474\n",
      "[900]\tvalid_0's stability_score: 0.684968\n",
      "[1000]\tvalid_0's stability_score: 0.685192\n",
      "[1100]\tvalid_0's stability_score: 0.685505\n",
      "[1200]\tvalid_0's stability_score: 0.686118\n",
      "[1300]\tvalid_0's stability_score: 0.685766\n",
      "Early stopping, best iteration is:\n",
      "[1246]\tvalid_0's stability_score: 0.686384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 03:54:52,487] Trial 24 finished with value: 0.6863836439607981 and parameters: {'num_leaves': 111, 'min_data_in_leaf': 529, 'learning_rate': 0.018696394902865537, 'lambda_l1': 1.2821900089281313e-05, 'lambda_l2': 0.002011136802779837, 'feature_fraction': 0.6527572309370945, 'bagging_fraction': 0.8694597053313206, 'bagging_freq': 3}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.658067\n",
      "[200]\tvalid_0's stability_score: 0.673022\n",
      "[300]\tvalid_0's stability_score: 0.678918\n",
      "[400]\tvalid_0's stability_score: 0.68099\n",
      "[500]\tvalid_0's stability_score: 0.682504\n",
      "[600]\tvalid_0's stability_score: 0.682679\n",
      "[700]\tvalid_0's stability_score: 0.682772\n",
      "[800]\tvalid_0's stability_score: 0.683133\n",
      "[900]\tvalid_0's stability_score: 0.682826\n",
      "Early stopping, best iteration is:\n",
      "[828]\tvalid_0's stability_score: 0.683358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 04:03:55,877] Trial 25 finished with value: 0.6833580504990489 and parameters: {'num_leaves': 227, 'min_data_in_leaf': 905, 'learning_rate': 0.030262166914726003, 'lambda_l1': 1.9281737116659274e-07, 'lambda_l2': 2.74804322716665, 'feature_fraction': 0.5762977987959823, 'bagging_fraction': 0.9299341575541547, 'bagging_freq': 0}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.668966\n",
      "[200]\tvalid_0's stability_score: 0.671395\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's stability_score: 0.672566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 04:06:43,595] Trial 26 finished with value: 0.672565826128374 and parameters: {'num_leaves': 167, 'min_data_in_leaf': 332, 'learning_rate': 0.08148015797025349, 'lambda_l1': 0.2685286193957568, 'lambda_l2': 0.044110050127223216, 'feature_fraction': 0.6783207103361464, 'bagging_fraction': 0.8974379182934872, 'bagging_freq': 2}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.649931\n",
      "[200]\tvalid_0's stability_score: 0.661703\n",
      "[300]\tvalid_0's stability_score: 0.671316\n",
      "[400]\tvalid_0's stability_score: 0.67686\n",
      "[500]\tvalid_0's stability_score: 0.680634\n",
      "[600]\tvalid_0's stability_score: 0.683112\n",
      "[700]\tvalid_0's stability_score: 0.684704\n",
      "[800]\tvalid_0's stability_score: 0.685731\n",
      "[900]\tvalid_0's stability_score: 0.686588\n",
      "[1000]\tvalid_0's stability_score: 0.687367\n",
      "[1100]\tvalid_0's stability_score: 0.687148\n",
      "Early stopping, best iteration is:\n",
      "[1048]\tvalid_0's stability_score: 0.687506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 04:19:18,032] Trial 27 finished with value: 0.6875060726074638 and parameters: {'num_leaves': 279, 'min_data_in_leaf': 735, 'learning_rate': 0.015098347645446007, 'lambda_l1': 0.001081767382483351, 'lambda_l2': 0.7651053862211805, 'feature_fraction': 0.5963108301047251, 'bagging_fraction': 0.836117107999597, 'bagging_freq': 4}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.657486\n",
      "[200]\tvalid_0's stability_score: 0.66622\n",
      "[300]\tvalid_0's stability_score: 0.665723\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid_0's stability_score: 0.667099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 04:23:16,091] Trial 28 finished with value: 0.6670986053189413 and parameters: {'num_leaves': 299, 'min_data_in_leaf': 52, 'learning_rate': 0.042563402407733764, 'lambda_l1': 0.014672690656013227, 'lambda_l2': 1.0517803209949552, 'feature_fraction': 0.6088945000943293, 'bagging_fraction': 0.8363090910540283, 'bagging_freq': 4}. Best is trial 11 with value: 0.6881509914750644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's stability_score: 0.64221\n",
      "[200]\tvalid_0's stability_score: 0.649151\n",
      "[300]\tvalid_0's stability_score: 0.654685\n",
      "[400]\tvalid_0's stability_score: 0.659165\n",
      "[500]\tvalid_0's stability_score: 0.66353\n",
      "[600]\tvalid_0's stability_score: 0.66721\n",
      "[700]\tvalid_0's stability_score: 0.670501\n",
      "[800]\tvalid_0's stability_score: 0.673444\n",
      "[900]\tvalid_0's stability_score: 0.675577\n",
      "[1000]\tvalid_0's stability_score: 0.677934\n",
      "[1100]\tvalid_0's stability_score: 0.679847\n",
      "[1200]\tvalid_0's stability_score: 0.681112\n",
      "[1300]\tvalid_0's stability_score: 0.682371\n",
      "[1400]\tvalid_0's stability_score: 0.68359\n",
      "[1500]\tvalid_0's stability_score: 0.684438\n",
      "[1600]\tvalid_0's stability_score: 0.685142\n",
      "[1700]\tvalid_0's stability_score: 0.685983\n",
      "[1800]\tvalid_0's stability_score: 0.68678\n",
      "[1900]\tvalid_0's stability_score: 0.687272\n",
      "[2000]\tvalid_0's stability_score: 0.687307\n",
      "[2100]\tvalid_0's stability_score: 0.687717\n",
      "[2200]\tvalid_0's stability_score: 0.68808\n",
      "[2300]\tvalid_0's stability_score: 0.688214\n",
      "[2400]\tvalid_0's stability_score: 0.688371\n",
      "Early stopping, best iteration is:\n",
      "[2347]\tvalid_0's stability_score: 0.688479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 04:48:28,857] Trial 29 finished with value: 0.6884790581859022 and parameters: {'num_leaves': 266, 'min_data_in_leaf': 731, 'learning_rate': 0.005901835737658873, 'lambda_l1': 0.0010634148733296234, 'lambda_l2': 5.835448638038832, 'feature_fraction': 0.5106655665557869, 'bagging_fraction': 0.7534811540507564, 'bagging_freq': 4}. Best is trial 29 with value: 0.6884790581859022.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  30\n",
      "Best trial:\n",
      "Value: 0.6884790581859022\n",
      "Params:\n",
      "num_leaves: 266\n",
      "min_data_in_leaf: 731\n",
      "learning_rate: 0.005901835737658873\n",
      "lambda_l1: 0.0010634148733296234\n",
      "lambda_l2: 5.835448638038832\n",
      "feature_fraction: 0.5106655665557869\n",
      "bagging_fraction: 0.7534811540507564\n",
      "bagging_freq: 4\n",
      "CPU times: total: 4d 2h 32min 41s\n",
      "Wall time: 5h 11min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Optuna study\n",
    "objective = lgbm_objective\n",
    "sampler = optuna.samplers.TPESampler(multivariate=True)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Show best results\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Number of finished trials: ', len(study.trials))\n",
    "print('Best trial:')\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "     print('{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 05:19:07,111] A new study created in memory with name: no-name-387bd64c-5ad7-43f1-9f56-972e3aeb3e3a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6855418\ttest: 0.6867555\tbest: 0.6867555 (0)\ttotal: 238ms\tremaining: 19m 49s\n",
      "50:\tlearn: 0.5135213\ttest: 0.5788604\tbest: 0.5788604 (50)\ttotal: 11.9s\tremaining: 19m 11s\n",
      "100:\tlearn: 0.4556692\ttest: 0.5743165\tbest: 0.5731124 (81)\ttotal: 23.1s\tremaining: 18m 41s\n",
      "150:\tlearn: 0.4115648\ttest: 0.5816089\tbest: 0.5731124 (81)\ttotal: 34.3s\tremaining: 18m 21s\n",
      "bestTest = 0.5731123517\n",
      "bestIteration = 81\n",
      "Shrink model to first 82 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 05:19:53,426] Trial 0 finished with value: 0.786542864918172 and parameters: {'learning_rate': 0.02853574682203823, 'l2_leaf_reg': 1.4611042789271413e-08, 'bagging_temperature': 8.670084696965707, 'random_strength': 3.842297697655097e-06, 'depth': 12, 'min_data_in_leaf': 36}. Best is trial 0 with value: 0.786542864918172.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Optuna study\n",
    "objective = cb_objective\n",
    "sampler = optuna.samplers.TPESampler(multivariate=True)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Show best results\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Number of finished trials: ', len(study.trials))\n",
    "print('Best trial:')\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "     print('{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Optuna study\n",
    "objective = xgb_objective\n",
    "sampler = optuna.samplers.TPESampler(multivariate=True)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Show best results\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Number of finished trials: ', len(study.trials))\n",
    "print('Best trial:')\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "     print('{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models' best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM best parameters\n",
    "lgbm_params = {\n",
    "    'n_estimators': 5000,\n",
    "    'num_leaves': 214, \n",
    "    'min_data_in_leaf': 1831, \n",
    "    'learning_rate': 0.018016752095308213, \n",
    "    'lambda_l1': 0.039542276491157664, \n",
    "    'lambda_l2': 0.0028839017821387612, \n",
    "    'feature_fraction': 0.8606139936996, \n",
    "    'bagging_fraction': 0.6505054297397137,\n",
    "    'bagging_freq': 4,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'None', # stability metric is used as eval_metric\n",
    "    'verbosity': -1,\n",
    "    'device': 'gpu',\n",
    "    'n_jobs': -1,\n",
    "    'max_bin': 255,\n",
    "    }\n",
    "\n",
    "#{'num_leaves': 266, 'min_data_in_leaf': 731, 'learning_rate': 0.005901835737658873, 'lambda_l1': 0.0010634148733296234,\n",
    "#'lambda_l2': 5.835448638038832, 'feature_fraction': 0.5106655665557869, 'bagging_fraction': 0.7534811540507564, 'bagging_freq': 4}\n",
    "# XGB best parameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 5000,\n",
    "    'learning_rate': 0.02563139404535397, \n",
    "    'max_depth': 17, \n",
    "    'min_child_weight': 1446, \n",
    "    'max_delta_setp': 0,\n",
    "    'subsample': 0.8564298564731834, \n",
    "    'colsample_bytree': 0.9235898601649664, \n",
    "    'reg_lambda': 1.1289608466365559e-08, \n",
    "    'reg_alpha': 4.911518931389214e-07, \n",
    "    'gamma': 0.00035869889925144383, \n",
    "    \n",
    "    'objective': 'binary:logistic',\n",
    "#     'eval_metric': 'auc',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'enable_categorical': True,\n",
    "    'verbosity': 0,\n",
    "    'device': 'cuda',\n",
    "    'n_jobs': -1,\n",
    "    }\n",
    "# CatBoost best parameters\n",
    "cb_params = {\n",
    "    'iterations': 5000,\n",
    "    'learning_rate': 0.08, \n",
    "    'l2_leaf_reg': 57.87508612048416, \n",
    "    'bagging_temperature': 0.737099486243173, \n",
    "    'random_strength': 0.0017723437301297412, \n",
    "    'depth': 6, \n",
    "    'min_data_in_leaf': 66,\n",
    "    \n",
    "    'objective': 'Logloss',\n",
    "    'eval_metric': 'Logloss', # auc and custom functions are not working on gpu\n",
    "    'verbose': 50, \n",
    "    'task_type': 'GPU',\n",
    "    'thread_count': -1,\n",
    "    }\n",
    "# {'learning_rate': 0.06141273570840567, 'l2_leaf_reg': 17.946406453936355, 'bagging_temperature': 1.2822776449667406, 'random_strength': 0.07236918779653291, 'depth': 10, 'min_data_in_leaf': 48}\n",
    "# {'learning_rate': 0.07746171181807705, 'l2_leaf_reg': 0.16856620630061742, 'bagging_temperature': 0.4939947354720309, 'random_strength': 0.10365883602086806, 'depth': 5, 'min_data_in_leaf': 90}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val, model_type, params):\n",
    "    \"\"\"\n",
    "    Train a specific model type with predefined parameters\n",
    "    \"\"\"\n",
    "    y_ratio = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "    params['scale_pos_weight'] = y_ratio\n",
    "    custom_metric = StabilityMetric(X_val)\n",
    "\n",
    "    if model_type == 'lgbm':\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  eval_metric=custom_metric.lgbm_stability_metric,\n",
    "                  callbacks=[log_evaluation(100), early_stopping(100)],\n",
    "                 )\n",
    "    elif model_type == 'xgb':\n",
    "        es = EarlyStopping(rounds=100, \n",
    "                           maximize=True,\n",
    "                           save_best=True,)\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  eval_metric=custom_metric.xgb_stability_metric,\n",
    "                  callbacks=[es],\n",
    "                 )\n",
    "    elif model_type == 'cb':\n",
    "        train_data = Pool(data=X_train, label=y_train, \n",
    "                          cat_features=enc_med_card_cols,\n",
    "                          )\n",
    "        val_data = Pool(data=X_val, label=y_val, \n",
    "                        cat_features=enc_med_card_cols,\n",
    "                        )\n",
    "        model = CatBoostClassifier(**params)  \n",
    "        model.fit(train_data,\n",
    "                  eval_set=val_data,\n",
    "                  early_stopping_rounds=100,\n",
    "                  )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train model\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "lgbm_model = train_model(X_train, y_train, X_val, y_val, 'lgbm', lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LGBM model features importance\n",
    "plot_importance(lgbm_model, \n",
    "                importance_type='gain', \n",
    "                max_num_features=20, \n",
    "                height=0.5,\n",
    "                grid=False,\n",
    "                precision=0,\n",
    "                )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and prepare test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del X_train, X_val, y_train, y_val\n",
    "\n",
    "# Read, preprocess and merge all the test files together\n",
    "X_test = read_prepare_all(test_path, files_dict)\n",
    "\n",
    "# Match X_test columns with X columns\n",
    "for col in X_structure.columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = X_structure[col]\n",
    "        print(f'{col} added to X_test')\n",
    "        \n",
    "X_test = X_test[X_structure.columns]\n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_in_batches(X_test, final_model, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process the test set and predict in batches\n",
    "    \"\"\"\n",
    "    num_samples = len(X_test)\n",
    "    num_batches = (num_samples // batch_size) + 1\n",
    "    y_pred = []\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "        X_batch = X_test.iloc[start_idx:end_idx]\n",
    "        X_batch = pd.DataFrame(\n",
    "            processor.transform(X_batch), \n",
    "            columns=processor.get_feature_names_out(),\n",
    "            index=X_batch.index)\n",
    "        X_batch[enc_med_card_cols] = (X_batch[enc_med_card_cols]\n",
    "                                      .astype(str).astype('category'))\n",
    "        batch_preds = []\n",
    "        \n",
    "        for model_type, model in final_model.items():\n",
    "            if 'single' in model_type:\n",
    "                batch_model_preds = model.predict_proba(X_batch)[:, 1]\n",
    "\n",
    "            elif 'ensemble' in model_type: \n",
    "                # Predict the average from a list of estimators\n",
    "                batch_model_preds = mean_predict_proba(\n",
    "                    X_batch, estimators=model)\n",
    "                \n",
    "            batch_preds.append(batch_model_preds)\n",
    "            \n",
    "        batch_preds = np.vstack(batch_preds)\n",
    "        y_pred.append(batch_preds) \n",
    "        \n",
    "    # The average from all predictions\n",
    "    y_pred = np.hstack(y_pred)\n",
    "    y_pred = np.mean(y_pred, axis=0) \n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "final_model = {\n",
    "    'single': lgbm_model,\n",
    "#     'single': xgb_model,\n",
    "#     'ensemble': lgbm_bag_week_model,\n",
    "}\n",
    "y_pred = predict_proba_in_batches(X_test, final_model, batch_size=10000)\n",
    "\n",
    "# Submit\n",
    "submit = pd.read_csv(main_path + 'sample_submission.csv')\n",
    "submit.score = y_pred\n",
    "\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "\n",
    "submit"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    },
    {
     "datasetId": 4580762,
     "sourceId": 7818749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4550700,
     "sourceId": 8411082,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
